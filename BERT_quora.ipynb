{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "2fd48234",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "import re\n",
    "from bs4 import BeautifulSoup\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "a240d6bf",
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "import nltk\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem.porter import PorterStemmer\n",
    "from nltk.stem import WordNetLemmatizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "0300b4af",
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "#used for performing lemmatization\n",
    "from nltk.stem import WordNetLemmatizer \n",
    "#used to remove repeating words like- of,we,the,them etc\n",
    "from nltk.corpus import stopwords"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "ba159263",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "ae2e7c0b",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv('train.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "c1848f84",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df.fillna('')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "b3e3f09c",
   "metadata": {},
   "outputs": [],
   "source": [
    "new_df = df.sample(300,random_state=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "8fc6ae21",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\Dhanamjaya\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\Dhanamjaya\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     C:\\Users\\Dhanamjaya\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "import nltk\n",
    "nltk.download('punkt')\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk import pos_tag\n",
    "nltk.download('stopwords')\n",
    "from nltk.corpus import stopwords\n",
    "nltk.download('wordnet')\n",
    "from nltk.corpus import wordnet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "6520f133",
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess(q):\n",
    "    \n",
    "    q = str(q).lower().strip()\n",
    "    \n",
    "    # Replace certain special characters with their string equivalents\n",
    "    q = q.replace('%', ' percent')\n",
    "    q = q.replace('$', ' dollar ')\n",
    "    q = q.replace('₹', ' rupee ')\n",
    "    q = q.replace('€', ' euro ')\n",
    "    q = q.replace('@', ' at ')\n",
    "    \n",
    "    # The pattern '[math]' appears around 900 times in the whole dataset.\n",
    "    q = q.replace('[math]', '')\n",
    "    \n",
    "    # Replacing some numbers with string equivalents (not perfect, can be done better to account for more cases)\n",
    "    q = q.replace(',000,000,000 ', 'b ')\n",
    "    q = q.replace(',000,000 ', 'm ')\n",
    "    q = q.replace(',000 ', 'k ')\n",
    "    q = re.sub(r'([0-9]+)000000000', r'\\1b', q)\n",
    "    q = re.sub(r'([0-9]+)000000', r'\\1m', q)\n",
    "    q = re.sub(r'([0-9]+)000', r'\\1k', q)\n",
    "    \n",
    "    # Decontracting words\n",
    "    # https://en.wikipedia.org/wiki/Wikipedia%3aList_of_English_contractions\n",
    "    # https://stackoverflow.com/a/19794953\n",
    "    contractions = { \n",
    "    \"ain't\": \"am not\",\n",
    "    \"aren't\": \"are not\",\n",
    "    \"can't\": \"can not\",\n",
    "    \"can't've\": \"can not have\",\n",
    "    \"'cause\": \"because\",\n",
    "    \"could've\": \"could have\",\n",
    "    \"couldn't\": \"could not\",\n",
    "    \"couldn't've\": \"could not have\",\n",
    "    \"didn't\": \"did not\",\n",
    "    \"doesn't\": \"does not\",\n",
    "    \"don't\": \"do not\",\n",
    "    \"hadn't\": \"had not\",\n",
    "    \"hadn't've\": \"had not have\",\n",
    "    \"hasn't\": \"has not\",\n",
    "    \"haven't\": \"have not\",\n",
    "    \"he'd\": \"he would\",\n",
    "    \"he'd've\": \"he would have\",\n",
    "    \"he'll\": \"he will\",\n",
    "    \"he'll've\": \"he will have\",\n",
    "    \"he's\": \"he is\",\n",
    "    \"how'd\": \"how did\",\n",
    "    \"how'd'y\": \"how do you\",\n",
    "    \"how'll\": \"how will\",\n",
    "    \"how's\": \"how is\",\n",
    "    \"i'd\": \"i would\",\n",
    "    \"i'd've\": \"i would have\",\n",
    "    \"i'll\": \"i will\",\n",
    "    \"i'll've\": \"i will have\",\n",
    "    \"i'm\": \"i am\",\n",
    "    \"i've\": \"i have\",\n",
    "    \"isn't\": \"is not\",\n",
    "    \"it'd\": \"it would\",\n",
    "    \"it'd've\": \"it would have\",\n",
    "    \"it'll\": \"it will\",\n",
    "    \"it'll've\": \"it will have\",\n",
    "    \"it's\": \"it is\",\n",
    "    \"let's\": \"let us\",\n",
    "    \"ma'am\": \"madam\",\n",
    "    \"mayn't\": \"may not\",\n",
    "    \"might've\": \"might have\",\n",
    "    \"mightn't\": \"might not\",\n",
    "    \"mightn't've\": \"might not have\",\n",
    "    \"must've\": \"must have\",\n",
    "    \"mustn't\": \"must not\",\n",
    "    \"mustn't've\": \"must not have\",\n",
    "    \"needn't\": \"need not\",\n",
    "    \"needn't've\": \"need not have\",\n",
    "    \"o'clock\": \"of the clock\",\n",
    "    \"oughtn't\": \"ought not\",\n",
    "    \"oughtn't've\": \"ought not have\",\n",
    "    \"shan't\": \"shall not\",\n",
    "    \"sha'n't\": \"shall not\",\n",
    "    \"shan't've\": \"shall not have\",\n",
    "    \"she'd\": \"she would\",\n",
    "    \"she'd've\": \"she would have\",\n",
    "    \"she'll\": \"she will\",\n",
    "    \"she'll've\": \"she will have\",\n",
    "    \"she's\": \"she is\",\n",
    "    \"should've\": \"should have\",\n",
    "    \"shouldn't\": \"should not\",\n",
    "    \"shouldn't've\": \"should not have\",\n",
    "    \"so've\": \"so have\",\n",
    "    \"so's\": \"so as\",\n",
    "    \"that'd\": \"that would\",\n",
    "    \"that'd've\": \"that would have\",\n",
    "    \"that's\": \"that is\",\n",
    "    \"there'd\": \"there would\",\n",
    "    \"there'd've\": \"there would have\",\n",
    "    \"there's\": \"there is\",\n",
    "    \"they'd\": \"they would\",\n",
    "    \"they'd've\": \"they would have\",\n",
    "    \"they'll\": \"they will\",\n",
    "    \"they'll've\": \"they will have\",\n",
    "    \"they're\": \"they are\",\n",
    "    \"they've\": \"they have\",\n",
    "    \"to've\": \"to have\",\n",
    "    \"wasn't\": \"was not\",\n",
    "    \"we'd\": \"we would\",\n",
    "    \"we'd've\": \"we would have\",\n",
    "    \"we'll\": \"we will\",\n",
    "    \"we'll've\": \"we will have\",\n",
    "    \"we're\": \"we are\",\n",
    "    \"we've\": \"we have\",\n",
    "    \"weren't\": \"were not\",\n",
    "    \"what'll\": \"what will\",\n",
    "    \"what'll've\": \"what will have\",\n",
    "    \"what're\": \"what are\",\n",
    "    \"what's\": \"what is\",\n",
    "    \"what've\": \"what have\",\n",
    "    \"when's\": \"when is\",\n",
    "    \"when've\": \"when have\",\n",
    "    \"where'd\": \"where did\",\n",
    "    \"where's\": \"where is\",\n",
    "    \"where've\": \"where have\",\n",
    "    \"who'll\": \"who will\",\n",
    "    \"who'll've\": \"who will have\",\n",
    "    \"who's\": \"who is\",\n",
    "    \"who've\": \"who have\",\n",
    "    \"why's\": \"why is\",\n",
    "    \"why've\": \"why have\",\n",
    "    \"will've\": \"will have\",\n",
    "    \"won't\": \"will not\",\n",
    "    \"won't've\": \"will not have\",\n",
    "    \"would've\": \"would have\",\n",
    "    \"wouldn't\": \"would not\",\n",
    "    \"wouldn't've\": \"would not have\",\n",
    "    \"y'all\": \"you all\",\n",
    "    \"y'all'd\": \"you all would\",\n",
    "    \"y'all'd've\": \"you all would have\",\n",
    "    \"y'all're\": \"you all are\",\n",
    "    \"y'all've\": \"you all have\",\n",
    "    \"you'd\": \"you would\",\n",
    "    \"you'd've\": \"you would have\",\n",
    "    \"you'll\": \"you will\",\n",
    "    \"you'll've\": \"you will have\",\n",
    "    \"you're\": \"you are\",\n",
    "    \"you've\": \"you have\"\n",
    "    }\n",
    "\n",
    "    q_decontracted = []\n",
    "\n",
    "    for word in q.split():\n",
    "        if word in contractions:\n",
    "            word = contractions[word]\n",
    "\n",
    "        q_decontracted.append(word)\n",
    "\n",
    "    q = ' '.join(q_decontracted)\n",
    "    q = q.replace(\"'ve\", \" have\")\n",
    "    q = q.replace(\"n't\", \" not\")\n",
    "    q = q.replace(\"'re\", \" are\")\n",
    "    q = q.replace(\"'ll\", \" will\")\n",
    "    \n",
    "    # Removing HTML tags\n",
    "    q = BeautifulSoup(q)\n",
    "    q = q.get_text()\n",
    "    \n",
    "    # Remove punctuations\n",
    "    pattern = re.compile('\\W')\n",
    "    q = re.sub(pattern, ' ', q).strip()\n",
    "    \n",
    "    # POS tagger dictionary\n",
    "    pos_dict = {'J':wordnet.ADJ, 'V':wordnet.VERB, 'N':wordnet.NOUN, 'R':wordnet.ADV}\n",
    "    def token_stop_pos(text):\n",
    "        tags = pos_tag(word_tokenize(text))\n",
    "        newlist = []\n",
    "        for word, tag in tags:\n",
    "            if word.lower() not in set(stopwords.words('english')):\n",
    "                newlist.append(tuple([word, pos_dict.get(tag[0])]))\n",
    "        return newlist\n",
    "\n",
    "    q=token_stop_pos(q)\n",
    "    def lemmatize(pos_data):\n",
    "        lemma_rew = \" \"\n",
    "        for word, pos in pos_data:\n",
    "            if not pos:\n",
    "                lemma = word\n",
    "                lemma_rew = lemma_rew + \" \" + lemma\n",
    "            else:\n",
    "                lemma = wordnet_lemmatizer.lemmatize(word, pos=pos)\n",
    "                lemma_rew = lemma_rew + \" \" + lemma\n",
    "        return lemma_rew\n",
    "    \n",
    "    q=lemmatize(q)\n",
    "    \n",
    "    \n",
    "    \n",
    "    \n",
    "    \n",
    "    \n",
    "    \n",
    "   \n",
    "    #tokenized_text = sent_tokenize(q)\n",
    "    #for n in tokenized_text:\n",
    "     #   wordsList = nltk.word_tokenize(n)\n",
    "    #wordsList = [w for w in wordsList if not w in stop_words]\n",
    "    \n",
    "    \n",
    "    #q = nltk.pos_tag(wordsList)\n",
    "    #print(tagged_words)\n",
    "    \n",
    "    #lemmatizer = WordNetLemmatizer()\n",
    "    #q = lemmatizer.lemmatize(q)\n",
    "\n",
    "    \n",
    "    return q"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "18492f38",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>qid1</th>\n",
       "      <th>qid2</th>\n",
       "      <th>question1</th>\n",
       "      <th>question2</th>\n",
       "      <th>is_duplicate</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>398782</th>\n",
       "      <td>398782</td>\n",
       "      <td>496695</td>\n",
       "      <td>532029</td>\n",
       "      <td>What is the best marketing automation tool for...</td>\n",
       "      <td>What is the best marketing automation tool for...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>115086</th>\n",
       "      <td>115086</td>\n",
       "      <td>187729</td>\n",
       "      <td>187730</td>\n",
       "      <td>I am poor but I want to invest. What should I do?</td>\n",
       "      <td>I am quite poor and I want to be very rich. Wh...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>327711</th>\n",
       "      <td>327711</td>\n",
       "      <td>454161</td>\n",
       "      <td>454162</td>\n",
       "      <td>I am from India and live abroad. I met a guy f...</td>\n",
       "      <td>T.I.E.T to Thapar University to Thapar Univers...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>367788</th>\n",
       "      <td>367788</td>\n",
       "      <td>498109</td>\n",
       "      <td>491396</td>\n",
       "      <td>Why do so many people in the U.S. hate the sou...</td>\n",
       "      <td>My boyfriend doesnt feel guilty when he hurts ...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>151235</th>\n",
       "      <td>151235</td>\n",
       "      <td>237843</td>\n",
       "      <td>50930</td>\n",
       "      <td>Consequences of Bhopal gas tragedy?</td>\n",
       "      <td>What was the reason behind the Bhopal gas trag...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20566</th>\n",
       "      <td>20566</td>\n",
       "      <td>38787</td>\n",
       "      <td>38788</td>\n",
       "      <td>Why were Hawx and Hawx 2 removed from Steam st...</td>\n",
       "      <td>Can you install game mods for games that were ...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>218491</th>\n",
       "      <td>218491</td>\n",
       "      <td>113473</td>\n",
       "      <td>325107</td>\n",
       "      <td>Which are the manga related episodes in 'Narut...</td>\n",
       "      <td>What filler episodes of Naruto should I watch ...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>195902</th>\n",
       "      <td>195902</td>\n",
       "      <td>296477</td>\n",
       "      <td>296478</td>\n",
       "      <td>Is castor oil enough for beard growth?</td>\n",
       "      <td>Does eucalyptus oil help with beard growth?</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13656</th>\n",
       "      <td>13656</td>\n",
       "      <td>26206</td>\n",
       "      <td>26207</td>\n",
       "      <td>Given an array of integers A and an integer k,...</td>\n",
       "      <td>I have an array of n integers. How many subarr...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>322080</th>\n",
       "      <td>322080</td>\n",
       "      <td>447851</td>\n",
       "      <td>419174</td>\n",
       "      <td>How does Trump believe that Philippine Preside...</td>\n",
       "      <td>Why did the Philippine president Rodrigo Duter...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>300 rows × 6 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "            id    qid1    qid2  \\\n",
       "398782  398782  496695  532029   \n",
       "115086  115086  187729  187730   \n",
       "327711  327711  454161  454162   \n",
       "367788  367788  498109  491396   \n",
       "151235  151235  237843   50930   \n",
       "...        ...     ...     ...   \n",
       "20566    20566   38787   38788   \n",
       "218491  218491  113473  325107   \n",
       "195902  195902  296477  296478   \n",
       "13656    13656   26206   26207   \n",
       "322080  322080  447851  419174   \n",
       "\n",
       "                                                question1  \\\n",
       "398782  What is the best marketing automation tool for...   \n",
       "115086  I am poor but I want to invest. What should I do?   \n",
       "327711  I am from India and live abroad. I met a guy f...   \n",
       "367788  Why do so many people in the U.S. hate the sou...   \n",
       "151235                Consequences of Bhopal gas tragedy?   \n",
       "...                                                   ...   \n",
       "20566   Why were Hawx and Hawx 2 removed from Steam st...   \n",
       "218491  Which are the manga related episodes in 'Narut...   \n",
       "195902             Is castor oil enough for beard growth?   \n",
       "13656   Given an array of integers A and an integer k,...   \n",
       "322080  How does Trump believe that Philippine Preside...   \n",
       "\n",
       "                                                question2  is_duplicate  \n",
       "398782  What is the best marketing automation tool for...             1  \n",
       "115086  I am quite poor and I want to be very rich. Wh...             0  \n",
       "327711  T.I.E.T to Thapar University to Thapar Univers...             0  \n",
       "367788  My boyfriend doesnt feel guilty when he hurts ...             0  \n",
       "151235  What was the reason behind the Bhopal gas trag...             0  \n",
       "...                                                   ...           ...  \n",
       "20566   Can you install game mods for games that were ...             0  \n",
       "218491  What filler episodes of Naruto should I watch ...             0  \n",
       "195902        Does eucalyptus oil help with beard growth?             0  \n",
       "13656   I have an array of n integers. How many subarr...             0  \n",
       "322080  Why did the Philippine president Rodrigo Duter...             1  \n",
       "\n",
       "[300 rows x 6 columns]"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "new_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "2ab10909",
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.stem import WordNetLemmatizer\n",
    "wordnet_lemmatizer = WordNetLemmatizer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "a6dae053",
   "metadata": {},
   "outputs": [],
   "source": [
    "new_df['question1'] = new_df['question1'].apply(preprocess)\n",
    "new_df['question2'] = new_df['question2'].apply(preprocess)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "22d83b21",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>qid1</th>\n",
       "      <th>qid2</th>\n",
       "      <th>question1</th>\n",
       "      <th>question2</th>\n",
       "      <th>is_duplicate</th>\n",
       "      <th>combine</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>398782</th>\n",
       "      <td>398782</td>\n",
       "      <td>496695</td>\n",
       "      <td>532029</td>\n",
       "      <td>best marketing automation tool small mid siz...</td>\n",
       "      <td>best marketing automation tool small mid siz...</td>\n",
       "      <td>1</td>\n",
       "      <td>best marketing automation tool small mid siz...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>115086</th>\n",
       "      <td>115086</td>\n",
       "      <td>187729</td>\n",
       "      <td>187730</td>\n",
       "      <td>poor want invest</td>\n",
       "      <td>quite poor want rich</td>\n",
       "      <td>0</td>\n",
       "      <td>poor want invest   quite poor want rich</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>327711</th>\n",
       "      <td>327711</td>\n",
       "      <td>454161</td>\n",
       "      <td>454162</td>\n",
       "      <td>india live abroad meet guy france party want...</td>\n",
       "      <td>e thapar university thapar university instit...</td>\n",
       "      <td>0</td>\n",
       "      <td>india live abroad meet guy france party want...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>367788</th>\n",
       "      <td>367788</td>\n",
       "      <td>498109</td>\n",
       "      <td>491396</td>\n",
       "      <td>many people u hate southern state</td>\n",
       "      <td>boyfriend doesnt feel guilty hurt cry tell k...</td>\n",
       "      <td>0</td>\n",
       "      <td>many people u hate southern state   boyfrien...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>151235</th>\n",
       "      <td>151235</td>\n",
       "      <td>237843</td>\n",
       "      <td>50930</td>\n",
       "      <td>consequence bhopal gas tragedy</td>\n",
       "      <td>reason behind bhopal gas tragedy</td>\n",
       "      <td>0</td>\n",
       "      <td>consequence bhopal gas tragedy   reason behi...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>244531</th>\n",
       "      <td>244531</td>\n",
       "      <td>145041</td>\n",
       "      <td>264664</td>\n",
       "      <td>kill snake friday belief kill snake friday c...</td>\n",
       "      <td>snake really take revenge</td>\n",
       "      <td>0</td>\n",
       "      <td>kill snake friday belief kill snake friday c...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16633</th>\n",
       "      <td>16633</td>\n",
       "      <td>31698</td>\n",
       "      <td>31699</td>\n",
       "      <td>royal family net gain net loss british taxpayer</td>\n",
       "      <td>british royal family think ok take taxpayer ...</td>\n",
       "      <td>0</td>\n",
       "      <td>royal family net gain net loss british taxpa...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>396316</th>\n",
       "      <td>396316</td>\n",
       "      <td>529365</td>\n",
       "      <td>529366</td>\n",
       "      <td>huge asteroid hit earth x year would able fi...</td>\n",
       "      <td>100 mile wide asteroid project hit earth 50 ...</td>\n",
       "      <td>0</td>\n",
       "      <td>huge asteroid hit earth x year would able fi...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>399493</th>\n",
       "      <td>399493</td>\n",
       "      <td>532771</td>\n",
       "      <td>456968</td>\n",
       "      <td>would happen woman take viagra</td>\n",
       "      <td>viagra really work</td>\n",
       "      <td>0</td>\n",
       "      <td>would happen woman take viagra   viagra real...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15268</th>\n",
       "      <td>15268</td>\n",
       "      <td>29183</td>\n",
       "      <td>29184</td>\n",
       "      <td>could improve love girlfriend</td>\n",
       "      <td>improve relationship girlfriend</td>\n",
       "      <td>0</td>\n",
       "      <td>could improve love girlfriend   improve rela...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "            id    qid1    qid2  \\\n",
       "398782  398782  496695  532029   \n",
       "115086  115086  187729  187730   \n",
       "327711  327711  454161  454162   \n",
       "367788  367788  498109  491396   \n",
       "151235  151235  237843   50930   \n",
       "244531  244531  145041  264664   \n",
       "16633    16633   31698   31699   \n",
       "396316  396316  529365  529366   \n",
       "399493  399493  532771  456968   \n",
       "15268    15268   29183   29184   \n",
       "\n",
       "                                                question1  \\\n",
       "398782    best marketing automation tool small mid siz...   \n",
       "115086                                   poor want invest   \n",
       "327711    india live abroad meet guy france party want...   \n",
       "367788                  many people u hate southern state   \n",
       "151235                     consequence bhopal gas tragedy   \n",
       "244531    kill snake friday belief kill snake friday c...   \n",
       "16633     royal family net gain net loss british taxpayer   \n",
       "396316    huge asteroid hit earth x year would able fi...   \n",
       "399493                     would happen woman take viagra   \n",
       "15268                       could improve love girlfriend   \n",
       "\n",
       "                                                question2  is_duplicate  \\\n",
       "398782    best marketing automation tool small mid siz...             1   \n",
       "115086                               quite poor want rich             0   \n",
       "327711    e thapar university thapar university instit...             0   \n",
       "367788    boyfriend doesnt feel guilty hurt cry tell k...             0   \n",
       "151235                   reason behind bhopal gas tragedy             0   \n",
       "244531                          snake really take revenge             0   \n",
       "16633     british royal family think ok take taxpayer ...             0   \n",
       "396316    100 mile wide asteroid project hit earth 50 ...             0   \n",
       "399493                                 viagra really work             0   \n",
       "15268                     improve relationship girlfriend             0   \n",
       "\n",
       "                                                  combine  \n",
       "398782    best marketing automation tool small mid siz...  \n",
       "115086            poor want invest   quite poor want rich  \n",
       "327711    india live abroad meet guy france party want...  \n",
       "367788    many people u hate southern state   boyfrien...  \n",
       "151235    consequence bhopal gas tragedy   reason behi...  \n",
       "244531    kill snake friday belief kill snake friday c...  \n",
       "16633     royal family net gain net loss british taxpa...  \n",
       "396316    huge asteroid hit earth x year would able fi...  \n",
       "399493    would happen woman take viagra   viagra real...  \n",
       "15268     could improve love girlfriend   improve rela...  "
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def concat(ser):#concatenate Question 1 & Question 2\n",
    "  print(ser['question1'])\n",
    "  return 1\n",
    "new_df['combine'] = new_df.apply(lambda ser: ser['question1'] + \" \" + ser['question2'],axis=1)\n",
    "new_df.head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "434ae0b5",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f2d702e7",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "c7c213a3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(300, 1421)\n"
     ]
    }
   ],
   "source": [
    "cv = TfidfVectorizer(max_features=50000)#Word to Vectors using Tf-Idf\n",
    "\n",
    "#Take combine questions data as X\n",
    "X = cv.fit_transform(new_df['combine'])\n",
    "y = np.array(new_df['is_duplicate'])\n",
    "print(X.shape)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "293775cd",
   "metadata": {},
   "source": [
    "# Handling unbalanced data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "cf376289",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: imblearn in c:\\users\\dhanamjaya\\appdata\\roaming\\python\\python39\\site-packages (0.0)Note: you may need to restart the kernel to use updated packages.\n",
      "Requirement already satisfied: imbalanced-learn in c:\\users\\dhanamjaya\\appdata\\roaming\\python\\python39\\site-packages (from imblearn) (0.9.1)\n",
      "Requirement already satisfied: numpy>=1.17.3 in c:\\users\\dhanamjaya\\anaconda3\\lib\\site-packages (from imbalanced-learn->imblearn) (1.21.5)\n",
      "Requirement already satisfied: joblib>=1.0.0 in c:\\users\\dhanamjaya\\anaconda3\\lib\\site-packages (from imbalanced-learn->imblearn) (1.1.0)\n",
      "Requirement already satisfied: threadpoolctl>=2.0.0 in c:\\users\\dhanamjaya\\anaconda3\\lib\\site-packages (from imbalanced-learn->imblearn) (2.2.0)\n",
      "Requirement already satisfied: scipy>=1.3.2 in c:\\users\\dhanamjaya\\anaconda3\\lib\\site-packages (from imbalanced-learn->imblearn) (1.7.3)\n",
      "Requirement already satisfied: scikit-learn>=1.1.0 in c:\\users\\dhanamjaya\\anaconda3\\lib\\site-packages (from imbalanced-learn->imblearn) (1.1.3)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "pip install imblearn --user"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "d1fea03e",
   "metadata": {},
   "outputs": [],
   "source": [
    "from imblearn.over_sampling import RandomOverSampler\n",
    "\n",
    "ros = RandomOverSampler(random_state=42)\n",
    "\n",
    "x_ros, y_ros = ros.fit_resample(X, y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "c902c7e6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(368, 1421) (20, 1421)\n"
     ]
    }
   ],
   "source": [
    "#Tarin-Test Spilt\n",
    "X_train, X_test, y_train, y_test = train_test_split(x_ros,y_ros,test_size=0.05)\n",
    "print(X_train.shape,X_test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "986bf682",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting transformers==2.11.0\n",
      "  Using cached transformers-2.11.0-py3-none-any.whl (674 kB)\n",
      "Collecting sacremoses\n",
      "  Using cached sacremoses-0.0.53-py3-none-any.whl\n",
      "Requirement already satisfied: packaging in c:\\users\\dhanamjaya\\anaconda3\\lib\\site-packages (from transformers==2.11.0) (21.3)\n",
      "Requirement already satisfied: requests in c:\\users\\dhanamjaya\\anaconda3\\lib\\site-packages (from transformers==2.11.0) (2.27.1)\n",
      "Requirement already satisfied: sentencepiece in c:\\users\\dhanamjaya\\anaconda3\\lib\\site-packages (from transformers==2.11.0) (0.1.97)\n",
      "Requirement already satisfied: regex!=2019.12.17 in c:\\users\\dhanamjaya\\anaconda3\\lib\\site-packages (from transformers==2.11.0) (2022.3.15)\n",
      "Requirement already satisfied: filelock in c:\\users\\dhanamjaya\\anaconda3\\lib\\site-packages (from transformers==2.11.0) (3.6.0)\n",
      "Collecting tokenizers==0.7.0\n",
      "  Using cached tokenizers-0.7.0.tar.gz (81 kB)\n",
      "  Installing build dependencies: started\n",
      "  Installing build dependencies: finished with status 'done'\n",
      "  Getting requirements to build wheel: started\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  ERROR: Command errored out with exit status 1:\n",
      "   command: 'C:\\Users\\Dhanamjaya\\anaconda3\\python.exe' 'C:\\Users\\Dhanamjaya\\anaconda3\\lib\\site-packages\\pip\\_vendor\\pep517\\in_process\\_in_process.py' build_wheel 'C:\\Users\\DHANAM~1\\AppData\\Local\\Temp\\tmp8alhum8l'\n",
      "       cwd: C:\\Users\\Dhanamjaya\\AppData\\Local\\Temp\\pip-install-12h69arc\\tokenizers_6013123ba14f4cff901c038018ecfa93\n",
      "  Complete output (46 lines):\n",
      "  running bdist_wheel\n",
      "  running build\n",
      "  running build_py\n",
      "  creating build\n",
      "  creating build\\lib.win-amd64-cpython-39\n",
      "  creating build\\lib.win-amd64-cpython-39\\tokenizers\n",
      "  copying tokenizers\\__init__.py -> build\\lib.win-amd64-cpython-39\\tokenizers\n",
      "  creating build\\lib.win-amd64-cpython-39\\tokenizers\\models\n",
      "  copying tokenizers\\models\\__init__.py -> build\\lib.win-amd64-cpython-39\\tokenizers\\models\n",
      "  creating build\\lib.win-amd64-cpython-39\\tokenizers\\decoders\n",
      "  copying tokenizers\\decoders\\__init__.py -> build\\lib.win-amd64-cpython-39\\tokenizers\\decoders\n",
      "  creating build\\lib.win-amd64-cpython-39\\tokenizers\\normalizers\n",
      "  copying tokenizers\\normalizers\\__init__.py -> build\\lib.win-amd64-cpython-39\\tokenizers\\normalizers\n",
      "  creating build\\lib.win-amd64-cpython-39\\tokenizers\\pre_tokenizers\n",
      "  copying tokenizers\\pre_tokenizers\\__init__.py -> build\\lib.win-amd64-cpython-39\\tokenizers\\pre_tokenizers\n",
      "  creating build\\lib.win-amd64-cpython-39\\tokenizers\\processors\n",
      "  copying tokenizers\\processors\\__init__.py -> build\\lib.win-amd64-cpython-39\\tokenizers\\processors\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Getting requirements to build wheel: finished with status 'done'\n",
      "    Preparing wheel metadata: started\n",
      "    Preparing wheel metadata: finished with status 'done'\n",
      "Requirement already satisfied: tqdm>=4.27 in c:\\users\\dhanamjaya\\anaconda3\\lib\\site-packages (from transformers==2.11.0) (4.64.0)\n",
      "Requirement already satisfied: numpy in c:\\users\\dhanamjaya\\anaconda3\\lib\\site-packages (from transformers==2.11.0) (1.21.5)\n",
      "Requirement already satisfied: colorama in c:\\users\\dhanamjaya\\anaconda3\\lib\\site-packages (from tqdm>=4.27->transformers==2.11.0) (0.4.4)\n",
      "Requirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in c:\\users\\dhanamjaya\\anaconda3\\lib\\site-packages (from packaging->transformers==2.11.0) (3.0.4)\n",
      "Requirement already satisfied: idna<4,>=2.5 in c:\\users\\dhanamjaya\\anaconda3\\lib\\site-packages (from requests->transformers==2.11.0) (3.3)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in c:\\users\\dhanamjaya\\anaconda3\\lib\\site-packages (from requests->transformers==2.11.0) (2021.10.8)\n",
      "Requirement already satisfied: urllib3<1.27,>=1.21.1 in c:\\users\\dhanamjaya\\anaconda3\\lib\\site-packages (from requests->transformers==2.11.0) (1.26.9)\n",
      "Requirement already satisfied: charset-normalizer~=2.0.0 in c:\\users\\dhanamjaya\\anaconda3\\lib\\site-packages (from requests->transformers==2.11.0) (2.0.4)\n",
      "Requirement already satisfied: joblib in c:\\users\\dhanamjaya\\anaconda3\\lib\\site-packages (from sacremoses->transformers==2.11.0) (1.1.0)\n",
      "Requirement already satisfied: click in c:\\users\\dhanamjaya\\anaconda3\\lib\\site-packages (from sacremoses->transformers==2.11.0) (8.0.4)\n",
      "Requirement already satisfied: six in c:\\users\\dhanamjaya\\anaconda3\\lib\\site-packages (from sacremoses->transformers==2.11.0) (1.16.0)\n",
      "Building wheels for collected packages: tokenizers\n",
      "  Building wheel for tokenizers (PEP 517): started\n",
      "  Building wheel for tokenizers (PEP 517): finished with status 'error'\n",
      "Failed to build tokenizers\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  creating build\\lib.win-amd64-cpython-39\\tokenizers\\trainers\n",
      "  copying tokenizers\\trainers\\__init__.py -> build\\lib.win-amd64-cpython-39\\tokenizers\\trainers\n",
      "  creating build\\lib.win-amd64-cpython-39\\tokenizers\\implementations\n",
      "  copying tokenizers\\implementations\\base_tokenizer.py -> build\\lib.win-amd64-cpython-39\\tokenizers\\implementations\n",
      "  copying tokenizers\\implementations\\bert_wordpiece.py -> build\\lib.win-amd64-cpython-39\\tokenizers\\implementations\n",
      "  copying tokenizers\\implementations\\byte_level_bpe.py -> build\\lib.win-amd64-cpython-39\\tokenizers\\implementations\n",
      "  copying tokenizers\\implementations\\char_level_bpe.py -> build\\lib.win-amd64-cpython-39\\tokenizers\\implementations\n",
      "  copying tokenizers\\implementations\\sentencepiece_bpe.py -> build\\lib.win-amd64-cpython-39\\tokenizers\\implementations\n",
      "  copying tokenizers\\implementations\\__init__.py -> build\\lib.win-amd64-cpython-39\\tokenizers\\implementations\n",
      "  copying tokenizers\\__init__.pyi -> build\\lib.win-amd64-cpython-39\\tokenizers\n",
      "  copying tokenizers\\models\\__init__.pyi -> build\\lib.win-amd64-cpython-39\\tokenizers\\models\n",
      "  copying tokenizers\\decoders\\__init__.pyi -> build\\lib.win-amd64-cpython-39\\tokenizers\\decoders\n",
      "  copying tokenizers\\normalizers\\__init__.pyi -> build\\lib.win-amd64-cpython-39\\tokenizers\\normalizers\n",
      "  copying tokenizers\\pre_tokenizers\\__init__.pyi -> build\\lib.win-amd64-cpython-39\\tokenizers\\pre_tokenizers\n",
      "  copying tokenizers\\processors\\__init__.pyi -> build\\lib.win-amd64-cpython-39\\tokenizers\\processors\n",
      "  copying tokenizers\\trainers\\__init__.pyi -> build\\lib.win-amd64-cpython-39\\tokenizers\\trainers\n",
      "  running build_ext\n",
      "  running build_rust\n",
      "  error: can't find Rust compiler\n",
      "  \n",
      "  If you are using an outdated pip version, it is possible a prebuilt wheel is available for this package but pip is not able to install from it. Installing from the wheel would avoid the need for a Rust compiler.\n",
      "  \n",
      "  To update pip, run:\n",
      "  \n",
      "      pip install --upgrade pip\n",
      "  \n",
      "  and then retry package installation.\n",
      "  \n",
      "  If you did intend to build this package from source, try installing a Rust compiler from your system package manager and ensure it is on the PATH during installation. Alternatively, rustup (available at https://rustup.rs) is the recommended way to download and update the Rust compiler toolchain.\n",
      "  ----------------------------------------\n",
      "  ERROR: Failed building wheel for tokenizers\n",
      "ERROR: Could not build wheels for tokenizers which use PEP 517 and cannot be installed directly\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import tensorflow as tf\n",
    "!pip install transformers==2.11.0\n",
    "import transformers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "f3504b54",
   "metadata": {},
   "outputs": [],
   "source": [
    "max_length = 128  # Maximum length of input sentence to the model.\n",
    "batch_size = 32\n",
    "epochs = 2\n",
    "\n",
    "# Labels in our dataset.\n",
    "labels = [1,0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "853fd87b",
   "metadata": {},
   "outputs": [],
   "source": [
    "new_df.dropna(axis=0,inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "3bdef445",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>qid1</th>\n",
       "      <th>qid2</th>\n",
       "      <th>question1</th>\n",
       "      <th>question2</th>\n",
       "      <th>is_duplicate</th>\n",
       "      <th>combine</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>115086</th>\n",
       "      <td>115086</td>\n",
       "      <td>187729</td>\n",
       "      <td>187730</td>\n",
       "      <td>poor want invest</td>\n",
       "      <td>quite poor want rich</td>\n",
       "      <td>0</td>\n",
       "      <td>poor want invest   quite poor want rich</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12536</th>\n",
       "      <td>12536</td>\n",
       "      <td>24147</td>\n",
       "      <td>24148</td>\n",
       "      <td>take nyquil sudafed time</td>\n",
       "      <td>danger take cymbalta valium time</td>\n",
       "      <td>0</td>\n",
       "      <td>take nyquil sudafed time   danger take cymba...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>119838</th>\n",
       "      <td>119838</td>\n",
       "      <td>34195</td>\n",
       "      <td>194481</td>\n",
       "      <td>logic behind facebook messenger list active ...</td>\n",
       "      <td>people friend see last active time messenger</td>\n",
       "      <td>0</td>\n",
       "      <td>logic behind facebook messenger list active ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>131852</th>\n",
       "      <td>131852</td>\n",
       "      <td>98316</td>\n",
       "      <td>195869</td>\n",
       "      <td>best haier refrigerator service center hyder...</td>\n",
       "      <td>best haier microwave oven service center hyd...</td>\n",
       "      <td>0</td>\n",
       "      <td>best haier refrigerator service center hyder...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>122112</th>\n",
       "      <td>122112</td>\n",
       "      <td>197727</td>\n",
       "      <td>197728</td>\n",
       "      <td>happn app know location app close io</td>\n",
       "      <td>happn app deal stalk</td>\n",
       "      <td>0</td>\n",
       "      <td>happn app know location app close io   happn...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "            id    qid1    qid2  \\\n",
       "115086  115086  187729  187730   \n",
       "12536    12536   24147   24148   \n",
       "119838  119838   34195  194481   \n",
       "131852  131852   98316  195869   \n",
       "122112  122112  197727  197728   \n",
       "\n",
       "                                                question1  \\\n",
       "115086                                   poor want invest   \n",
       "12536                            take nyquil sudafed time   \n",
       "119838    logic behind facebook messenger list active ...   \n",
       "131852    best haier refrigerator service center hyder...   \n",
       "122112               happn app know location app close io   \n",
       "\n",
       "                                                question2  is_duplicate  \\\n",
       "115086                               quite poor want rich             0   \n",
       "12536                    danger take cymbalta valium time             0   \n",
       "119838       people friend see last active time messenger             0   \n",
       "131852    best haier microwave oven service center hyd...             0   \n",
       "122112                               happn app deal stalk             0   \n",
       "\n",
       "                                                  combine  \n",
       "115086            poor want invest   quite poor want rich  \n",
       "12536     take nyquil sudafed time   danger take cymba...  \n",
       "119838    logic behind facebook messenger list active ...  \n",
       "131852    best haier refrigerator service center hyder...  \n",
       "122112    happn app know location app close io   happn...  "
      ]
     },
     "execution_count": 53,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mask = np.random.rand(len(new_df)) < 0.7\n",
    "train_df = new_df[mask]\n",
    "not_train = new_df[~mask]\n",
    "\n",
    "#create mask for val-test distribution\n",
    "mask = np.random.rand(len(not_train)) < 0.5\n",
    "test_df = not_train[mask]\n",
    "val_df = not_train[~mask]\n",
    "val_df.head(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "ad114a01",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total train samples : 190\n",
      "Total validation samples: 62\n",
      "Total test samples: 48\n"
     ]
    }
   ],
   "source": [
    "print(f\"Total train samples : {train_df.shape[0]}\")\n",
    "print(f\"Total validation samples: {val_df.shape[0]}\")\n",
    "print(f\"Total test samples: {test_df.shape[0]}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "d1a5e1e6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Target Distribution\n",
      "0    123\n",
      "1     67\n",
      "Name: is_duplicate, dtype: int64\n",
      "Validation Target Distribution\n",
      "0    46\n",
      "1    16\n",
      "Name: is_duplicate, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "print(\"Train Target Distribution\")\n",
    "print(train_df.is_duplicate.value_counts())\n",
    "\n",
    "print(\"Validation Target Distribution\")\n",
    "print(val_df.is_duplicate.value_counts())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "5a2be2c4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "y_train.shape:(190, 2)\n",
      "y_val.shape:(62, 2)\n",
      "y_test.shape:(48, 2)\n"
     ]
    }
   ],
   "source": [
    "y_train = tf.keras.utils.to_categorical(train_df.is_duplicate, num_classes=2)#One hot encodding representation\n",
    "print(f\"y_train.shape:{y_train.shape}\")\n",
    "\n",
    "y_val = tf.keras.utils.to_categorical(val_df.is_duplicate, num_classes=2)\n",
    "print(f\"y_val.shape:{y_val.shape}\")\n",
    "\n",
    "y_test = tf.keras.utils.to_categorical(test_df.is_duplicate, num_classes=2)\n",
    "print(f\"y_test.shape:{y_test.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "4e8ff722",
   "metadata": {},
   "outputs": [],
   "source": [
    "class BertSemanticDataGenerator(tf.keras.utils.Sequence):\n",
    "    \"\"\"Generates batches of data.\n",
    "\n",
    "    Args:\n",
    "        sentence_pairs: Array of premise and hypothesis input sentences.\n",
    "        labels: Array of labels.\n",
    "        batch_size: Integer batch size.\n",
    "        shuffle: boolean, whether to shuffle the data.\n",
    "        include_targets: boolean, whether to incude the labels.\n",
    "\n",
    "    Returns:\n",
    "        Tuples `([input_ids, attention_mask, `token_type_ids], labels)`\n",
    "        (or just `[input_ids, attention_mask, `token_type_ids]`\n",
    "         if `include_targets=False`)\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(\n",
    "        self,\n",
    "        sentence_pairs,\n",
    "        labels,\n",
    "        batch_size=batch_size,\n",
    "        shuffle=True,\n",
    "        include_targets=True,\n",
    "    ):\n",
    "        self.sentence_pairs = sentence_pairs\n",
    "        self.labels = labels\n",
    "        self.shuffle = shuffle\n",
    "        self.batch_size = batch_size\n",
    "        self.include_targets = include_targets\n",
    "        \n",
    "        # Load our BERT Tokenizer to encode the text.\n",
    "        # We will use base-base-uncased pretrained model.\n",
    "        self.tokenizer = transformers.BertTokenizer.from_pretrained(\n",
    "            \"bert-base-uncased\", do_lower_case=True\n",
    "        )\n",
    "        self.indexes = np.arange(len(self.sentence_pairs))\n",
    "        self.on_epoch_end()\n",
    "\n",
    "    def __len__(self):\n",
    "        # Denotes the number of batches per epoch.\n",
    "        return len(self.sentence_pairs) // self.batch_size\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        # Retrieves the batch of index.\n",
    "        indexes = self.indexes[idx * self.batch_size : (idx + 1) * self.batch_size]\n",
    "        sentence_pairs = self.sentence_pairs[indexes]\n",
    "\n",
    "        # With BERT tokenizer's batch_encode_plus batch of both the sentences are\n",
    "        # encoded together and separated by [SEP] token.\n",
    "        encoded = self.tokenizer.batch_encode_plus(\n",
    "            sentence_pairs.tolist(),\n",
    "            add_special_tokens=True,\n",
    "            max_length=max_length,\n",
    "            return_attention_mask=True,\n",
    "            return_token_type_ids=True,\n",
    "            pad_to_max_length=True,\n",
    "            return_tensors=\"tf\",\n",
    "        )\n",
    "\n",
    "        # Convert batch of encoded features to numpy array.\n",
    "        input_ids = np.array(encoded[\"input_ids\"], dtype=\"int32\")\n",
    "        attention_masks = np.array(encoded[\"attention_mask\"], dtype=\"int32\")\n",
    "        token_type_ids = np.array(encoded[\"token_type_ids\"], dtype=\"int32\")\n",
    "\n",
    "        # Set to true if data generator is used for training/validation.\n",
    "        if self.include_targets:\n",
    "            labels = np.array(self.labels[indexes], dtype=\"int32\")\n",
    "            return [input_ids, attention_masks, token_type_ids], labels\n",
    "        else:\n",
    "            return [input_ids, attention_masks, token_type_ids]\n",
    "\n",
    "    def on_epoch_end(self):\n",
    "        # Shuffle indexes after each epoch if shuffle is set to True.\n",
    "        if self.shuffle:\n",
    "            np.random.RandomState(42).shuffle(self.indexes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "id": "0b680489",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some layers from the model checkpoint at bert-base-uncased were not used when initializing TFBertModel: ['nsp___cls', 'mlm___cls']\n",
      "- This IS expected if you are initializing TFBertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing TFBertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "All the layers of TFBertModel were initialized from the model checkpoint at bert-base-uncased.\n",
      "If your task is similar to the task the model of the checkpoint was trained on, you can already use TFBertModel for predictions without further training.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Strategy: <tensorflow.python.distribute.mirrored_strategy.MirroredStrategy object at 0x000002B0E2B34E20>\n",
      "Model: \"model_1\"\n",
      "__________________________________________________________________________________________________\n",
      " Layer (type)                   Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      " input_ids (InputLayer)         [(None, 128)]        0           []                               \n",
      "                                                                                                  \n",
      " attention_masks (InputLayer)   [(None, 128)]        0           []                               \n",
      "                                                                                                  \n",
      " token_type_ids (InputLayer)    [(None, 128)]        0           []                               \n",
      "                                                                                                  \n",
      " tf_bert_model_8 (TFBertModel)  TFBaseModelOutputWi  109482240   ['input_ids[0][0]',              \n",
      "                                thPoolingAndCrossAt               'attention_masks[0][0]',        \n",
      "                                tentions(last_hidde               'token_type_ids[0][0]']         \n",
      "                                n_state=(None, 128,                                               \n",
      "                                 768),                                                            \n",
      "                                 pooler_output=(Non                                               \n",
      "                                e, 768),                                                          \n",
      "                                 past_key_values=No                                               \n",
      "                                ne, hidden_states=N                                               \n",
      "                                one, attentions=Non                                               \n",
      "                                e, cross_attentions                                               \n",
      "                                =None)                                                            \n",
      "                                                                                                  \n",
      " bidirectional_4 (Bidirectional  (None, 128, 128)    426496      ['tf_bert_model_8[0][0]']        \n",
      " )                                                                                                \n",
      "                                                                                                  \n",
      " global_average_pooling1d_1 (Gl  (None, 128)         0           ['bidirectional_4[0][0]']        \n",
      " obalAveragePooling1D)                                                                            \n",
      "                                                                                                  \n",
      " global_max_pooling1d_1 (Global  (None, 128)         0           ['bidirectional_4[0][0]']        \n",
      " MaxPooling1D)                                                                                    \n",
      "                                                                                                  \n",
      " concatenate_1 (Concatenate)    (None, 256)          0           ['global_average_pooling1d_1[0][0\n",
      "                                                                 ]',                              \n",
      "                                                                  'global_max_pooling1d_1[0][0]'] \n",
      "                                                                                                  \n",
      " dropout_334 (Dropout)          (None, 256)          0           ['concatenate_1[0][0]']          \n",
      "                                                                                                  \n",
      " dense_1 (Dense)                (None, 2)            514         ['dropout_334[0][0]']            \n",
      "                                                                                                  \n",
      "==================================================================================================\n",
      "Total params: 109,909,250\n",
      "Trainable params: 427,010\n",
      "Non-trainable params: 109,482,240\n",
      "__________________________________________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "with strategy.scope():\n",
    "    # Encoded token ids from BERT tokenizer.\n",
    "    \n",
    "    input_ids = tf.keras.layers.Input(\n",
    "        shape=(max_length,), dtype=tf.int32, name=\"input_ids\"\n",
    "    )\n",
    "    # Attention masks indicates to the model which tokens should be attended to.\n",
    "    attention_masks = tf.keras.layers.Input(\n",
    "        shape=(max_length,), dtype=tf.int32, name=\"attention_masks\"\n",
    "    )\n",
    "    # Token type ids are binary masks identifying different sequences in the model.\n",
    "    token_type_ids = tf.keras.layers.Input(\n",
    "        shape=(max_length,), dtype=tf.int32, name=\"token_type_ids\"\n",
    "    )\n",
    "    # Loading pretrained BERT model.\n",
    "    bert_model = transformers.TFBertModel.from_pretrained(\"bert-base-uncased\")\n",
    "    # Freeze the BERT model to reuse the pretrained features without modifying them.\n",
    "    bert_model.trainable = False\n",
    "\n",
    "    output = bert_model(\n",
    "        input_ids, attention_mask=attention_masks, token_type_ids=token_type_ids)\n",
    "\n",
    "    bi_lstm = tf.keras.layers.Bidirectional(\n",
    "        tf.keras.layers.LSTM(64, return_sequences=True)\n",
    "    )(output['last_hidden_state'])\n",
    "    # Applying hybrid pooling approach to bi_lstm sequence output.\n",
    "    avg_pool = tf.keras.layers.GlobalAveragePooling1D()(bi_lstm)\n",
    "    max_pool = tf.keras.layers.GlobalMaxPooling1D()(bi_lstm)\n",
    "    concat = tf.keras.layers.concatenate([avg_pool, max_pool])\n",
    "    dropout = tf.keras.layers.Dropout(0.3)(concat)\n",
    "    output = tf.keras.layers.Dense(2, activation=\"softmax\")(dropout)\n",
    "    model = tf.keras.models.Model(\n",
    "        inputs=[input_ids, attention_masks, token_type_ids], outputs=output\n",
    "    )\n",
    "\n",
    "    model.compile(\n",
    "        optimizer=tf.keras.optimizers.Adam(),\n",
    "        loss=\"categorical_crossentropy\",\n",
    "        metrics=[\"acc\"],\n",
    "    )\n",
    "\n",
    "print(f\"Strategy: {strategy}\")\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3a60b86c",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "id": "d15c9b09",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data = BertSemanticDataGenerator(\n",
    "    train_df[[\"question1\", \"question2\"]].values.astype(\"str\"),\n",
    "    y_train,\n",
    "    batch_size=batch_size,\n",
    "    shuffle=True,\n",
    ")\n",
    "val_data = BertSemanticDataGenerator(\n",
    "    val_df[[\"question1\", \"question2\"]].values.astype(\"str\"),\n",
    "    y_val,\n",
    "    batch_size=batch_size,\n",
    "    shuffle=False,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "id": "06669961",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/2\n",
      "5/5 [==============================] - ETA: 0s - loss: 0.6896 - acc: 0.6187 "
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "5/5 [==============================] - 132s 21s/step - loss: 0.6896 - acc: 0.6187 - val_loss: 0.5480 - val_acc: 0.8750\n",
      "Epoch 2/2\n",
      "5/5 [==============================] - 93s 19s/step - loss: 0.5931 - acc: 0.6875 - val_loss: 0.5415 - val_acc: 0.7500\n"
     ]
    }
   ],
   "source": [
    "history = model.fit(\n",
    "    train_data,\n",
    "    validation_data=val_data,\n",
    "    epochs=epochs,\n",
    "     use_multiprocessing=True,\n",
    "    workers=-1,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "id": "2fb6b0ef",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model_1\"\n",
      "__________________________________________________________________________________________________\n",
      " Layer (type)                   Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      " input_ids (InputLayer)         [(None, 128)]        0           []                               \n",
      "                                                                                                  \n",
      " attention_masks (InputLayer)   [(None, 128)]        0           []                               \n",
      "                                                                                                  \n",
      " token_type_ids (InputLayer)    [(None, 128)]        0           []                               \n",
      "                                                                                                  \n",
      " tf_bert_model_8 (TFBertModel)  TFBaseModelOutputWi  109482240   ['input_ids[0][0]',              \n",
      "                                thPoolingAndCrossAt               'attention_masks[0][0]',        \n",
      "                                tentions(last_hidde               'token_type_ids[0][0]']         \n",
      "                                n_state=(None, 128,                                               \n",
      "                                 768),                                                            \n",
      "                                 pooler_output=(Non                                               \n",
      "                                e, 768),                                                          \n",
      "                                 past_key_values=No                                               \n",
      "                                ne, hidden_states=N                                               \n",
      "                                one, attentions=Non                                               \n",
      "                                e, cross_attentions                                               \n",
      "                                =None)                                                            \n",
      "                                                                                                  \n",
      " bidirectional_4 (Bidirectional  (None, 128, 128)    426496      ['tf_bert_model_8[0][0]']        \n",
      " )                                                                                                \n",
      "                                                                                                  \n",
      " global_average_pooling1d_1 (Gl  (None, 128)         0           ['bidirectional_4[0][0]']        \n",
      " obalAveragePooling1D)                                                                            \n",
      "                                                                                                  \n",
      " global_max_pooling1d_1 (Global  (None, 128)         0           ['bidirectional_4[0][0]']        \n",
      " MaxPooling1D)                                                                                    \n",
      "                                                                                                  \n",
      " concatenate_1 (Concatenate)    (None, 256)          0           ['global_average_pooling1d_1[0][0\n",
      "                                                                 ]',                              \n",
      "                                                                  'global_max_pooling1d_1[0][0]'] \n",
      "                                                                                                  \n",
      " dropout_334 (Dropout)          (None, 256)          0           ['concatenate_1[0][0]']          \n",
      "                                                                                                  \n",
      " dense_1 (Dense)                (None, 2)            514         ['dropout_334[0][0]']            \n",
      "                                                                                                  \n",
      "==================================================================================================\n",
      "Total params: 109,909,250\n",
      "Trainable params: 109,909,250\n",
      "Non-trainable params: 0\n",
      "__________________________________________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "bert_model.trainable = True\n",
    "# Recompile the model to make the change effective.\n",
    "model.compile(\n",
    "    optimizer=tf.keras.optimizers.Adam(1e-5),\n",
    "    loss=\"categorical_crossentropy\",\n",
    "    metrics=[\"accuracy\"],\n",
    ")\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "id": "76c9dd9b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/2\n",
      "WARNING:tensorflow:Gradients do not exist for variables ['tf_bert_model_8/bert/pooler/dense/kernel:0', 'tf_bert_model_8/bert/pooler/dense/bias:0'] when minimizing the loss. If you're using `model.compile()`, did you forget to provide a `loss` argument?\n",
      "WARNING:tensorflow:Gradients do not exist for variables ['tf_bert_model_8/bert/pooler/dense/kernel:0', 'tf_bert_model_8/bert/pooler/dense/bias:0'] when minimizing the loss. If you're using `model.compile()`, did you forget to provide a `loss` argument?\n",
      "5/5 [==============================] - 1255s 242s/step - loss: 0.4868 - accuracy: 0.7875 - val_loss: 0.4725 - val_accuracy: 0.8438\n",
      "Epoch 2/2\n",
      "5/5 [==============================] - 1296s 300s/step - loss: 0.4411 - accuracy: 0.8125 - val_loss: 0.4810 - val_accuracy: 0.8438\n"
     ]
    }
   ],
   "source": [
    "history = model.fit(\n",
    "    train_data,\n",
    "    validation_data=val_data,\n",
    "    epochs=epochs,\n",
    "    use_multiprocessing=True,\n",
    "    workers=-1,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "id": "4023793b",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1/1 [==============================] - 14s 14s/step - loss: 0.5851 - accuracy: 0.7500\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[0.5850765705108643, 0.75]"
      ]
     },
     "execution_count": 68,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_data = BertSemanticDataGenerator(\n",
    "    test_df[[\"question1\", \"question2\"]].values.astype(\"str\"),\n",
    "    y_test,\n",
    "    batch_size=batch_size,\n",
    "    shuffle=False,\n",
    ")\n",
    "model.evaluate(test_data, verbose=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "id": "003e3832",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "([array([[  101,  2116,  2111, ...,     0,     0,     0],\n",
       "         [  101,  6014,  6235, ...,     0,     0,     0],\n",
       "         [  101,  2131, 11924, ...,     0,     0,     0],\n",
       "         ...,\n",
       "         [  101,  2132,  9864, ...,     0,     0,     0],\n",
       "         [  101,  4578,  3623, ...,     0,     0,     0],\n",
       "         [  101,  2327,  9927, ...,     0,     0,     0]]),\n",
       "  array([[1, 1, 1, ..., 0, 0, 0],\n",
       "         [1, 1, 1, ..., 0, 0, 0],\n",
       "         [1, 1, 1, ..., 0, 0, 0],\n",
       "         ...,\n",
       "         [1, 1, 1, ..., 0, 0, 0],\n",
       "         [1, 1, 1, ..., 0, 0, 0],\n",
       "         [1, 1, 1, ..., 0, 0, 0]]),\n",
       "  array([[0, 0, 0, ..., 0, 0, 0],\n",
       "         [0, 0, 0, ..., 0, 0, 0],\n",
       "         [0, 0, 0, ..., 0, 0, 0],\n",
       "         ...,\n",
       "         [0, 0, 0, ..., 0, 0, 0],\n",
       "         [0, 0, 0, ..., 0, 0, 0],\n",
       "         [0, 0, 0, ..., 0, 0, 0]])],\n",
       " array([[1, 0],\n",
       "        [0, 1],\n",
       "        [1, 0],\n",
       "        [1, 0],\n",
       "        [1, 0],\n",
       "        [0, 1],\n",
       "        [0, 1],\n",
       "        [1, 0],\n",
       "        [1, 0],\n",
       "        [0, 1],\n",
       "        [1, 0],\n",
       "        [0, 1],\n",
       "        [1, 0],\n",
       "        [0, 1],\n",
       "        [1, 0],\n",
       "        [0, 1],\n",
       "        [1, 0],\n",
       "        [1, 0],\n",
       "        [0, 1],\n",
       "        [0, 1],\n",
       "        [1, 0],\n",
       "        [0, 1],\n",
       "        [1, 0],\n",
       "        [1, 0],\n",
       "        [0, 1],\n",
       "        [0, 1],\n",
       "        [0, 1],\n",
       "        [1, 0],\n",
       "        [0, 1],\n",
       "        [1, 0],\n",
       "        [0, 1],\n",
       "        [0, 1]]))"
      ]
     },
     "execution_count": 77,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_data[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "id": "9c3bbb38",
   "metadata": {},
   "outputs": [],
   "source": [
    "def check_similarity(sentence1, sentence2):\n",
    "  sentence_pairs = np.array([[str(sentence1), str(sentence2)]])\n",
    "  test_data = BertSemanticDataGenerator(\n",
    "      sentence_pairs, labels=None, batch_size=1, shuffle=False, include_targets=False,\n",
    "  )\n",
    "  print(test_data)\n",
    "  proba = model.predict(test_data[0])\n",
    "  idx = np.argmax(proba)\n",
    "  #proba = f\"{proba[idx]: .2f}%\"\n",
    "  pred = labels[idx]\n",
    "  return pred, proba"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "id": "f101b7a9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  programmable device use run walk track\n",
      "  people usually conscious awareness\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<__main__.BertSemanticDataGenerator object at 0x000002B0C3416820>\n",
      "1/1 [==============================] - 1s 620ms/step\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(1, array([[0.87927085, 0.12072913]], dtype=float32))"
      ]
     },
     "execution_count": 90,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ind = np.random.randint(0,20)\n",
    "#Non-Duplicate Questions\n",
    "q1 = test_df[test_df[\"is_duplicate\"] == 0].iloc[ind]['question1']\n",
    "q2 = test_df[test_df[\"is_duplicate\"] == 0].iloc[ind]['question2']\n",
    "print(q1+\"\\n\"+q2)\n",
    "check_similarity(q1,q2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2a58acd6",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
